{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH25O+776/UBC9+T0JFveh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/ASL2/blob/main/ASL99.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install compatible versions of dependent libraries"
      ],
      "metadata": {
        "id": "UCPlIsNhCmnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages that may cause conflicts\n",
        "!pip uninstall -y tensorflow tensorflow-metadata\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install mediapipe==0.10.15\n",
        "!pip install protobuf==3.20.3\n",
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "HtUYhaNRAY8V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddf3a58f-2a34-4989-df00-3cb0f03ae6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Found existing installation: tensorflow-metadata 1.13.1\n",
            "Uninstalling tensorflow-metadata-1.13.1:\n",
            "  Successfully uninstalled tensorflow-metadata-1.13.1\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.67.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.12.1)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.2)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorflow-estimator, tensorboard-data-server, protobuf, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.7 requires tensorflow-metadata, which is not installed.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.71.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.26.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ee305e93574d48fa95072d77d18c1400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe==0.10.15\n",
            "  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe==0.10.15) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.15)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.15)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.15) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe==0.10.15) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe==0.10.15) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe==0.10.15) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe==0.10.15) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.15) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.15) (1.16.0)\n",
            "Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.7 requires tensorflow-metadata, which is not installed.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.5 sounddevice-0.5.1\n",
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.7 requires tensorflow-metadata, which is not installed.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "mediapipe 0.10.15 requires protobuf<5,>=4.25.3, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "e9002fb5a0fe47d3adcb00549b564bff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.11.0\n",
            "MediaPipe version: 0.10.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "uIThM0T7CHI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53daa11-7fc5-40af-eb8f-081c36e45d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.11.0\n",
            "MediaPipe version: 0.10.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset"
      ],
      "metadata": {
        "id": "pUOBb1nVwTyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload archive.zip to /content/sample_data/"
      ],
      "metadata": {
        "id": "5eXt1gToxNf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the Dataset"
      ],
      "metadata": {
        "id": "qFMLQcgOwgQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLe8-fFDehIb"
      },
      "outputs": [],
      "source": [
        "!unzip /content/sample_data/archive.zip -d /content/asl_alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **not required**# Organizing and Sorting the data"
      ],
      "metadata": {
        "id": "JIZYrVAK_Ol-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders for each class in the test directory and move the images into their respective folders. For training data its already sorted."
      ],
      "metadata": {
        "id": "GSL886HUE0xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Define the classes\n",
        "class_names = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I',\n",
        "               'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
        "               'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n",
        "\n",
        "# Create directories for each class in the test directory\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Move each test image into the appropriate class folder\n",
        "for image_name in os.listdir(test_dir):\n",
        "    if '_test.jpg' in image_name:\n",
        "        class_name = image_name.split('_')[0]  # Get the class from the image name\n",
        "        shutil.move(os.path.join(test_dir, image_name),\n",
        "                    os.path.join(test_dir, class_name, image_name))"
      ],
      "metadata": {
        "id": "THuxXIF__lSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not Required** Load the training and testing datasets"
      ],
      "metadata": {
        "id": "G9wVXmEywvdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Size\n",
        "Definition: The batch size refers to the number of training examples used in one iteration of training.\n",
        "Purpose: Instead of updating the model weights after each individual sample, which can be computationally expensive and lead to noisy updates, the model processes a batch of samples and computes the gradient based on that batch. This approach helps to stabilize the training process and can lead to faster convergence.\n",
        "Common Values: Common batch sizes are powers of 2 (like 32, 64, 128) since they can be more efficient for hardware acceleration (e.g., GPUs).\n",
        "2. Reproducibility\n",
        "Definition: Reproducibility refers to the ability to achieve the same results when you run the experiment multiple times under the same conditions.\n",
        "Purpose: In machine learning, due to the stochastic nature of algorithms (like random initialization of weights or shuffling of data), you might get different results on different runs. Setting a random seed (like seed=123 in the code) ensures that the random processes in your code (e.g., shuffling data or initializing weights) are the same every time you run the model. This way, you can replicate results and debug issues more easily.\n",
        "3. Shuffle the Dataset\n",
        "Definition: Shuffling the dataset means randomizing the order of the data samples before they are fed into the model for training.\n",
        "Purpose: Shuffling is important to ensure that the model does not learn any unintended patterns based on the order of the data (e.g., if all images of one class appear consecutively). Randomizing the input helps the model generalize better and improves performance."
      ],
      "metadata": {
        "id": "MHsYc1QK8n53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "\n",
        "# Set image parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 200, 200\n",
        "BATCH_SIZE = 32  # Adjust as needed\n",
        "SEED = 123  # Set the seed for reproducibility\n",
        "\n",
        "# Load training data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Load testing data\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Check the class names\n",
        "class_names = train_ds.class_names\n",
        "print(\"Class names:\", class_names)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KLiOxsKZyF9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **not required** missing testing image"
      ],
      "metadata": {
        "id": "is0RhxiasJZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Count test images per class\n",
        "for class_name in class_names:\n",
        "    class_path = os.path.join(test_dir, class_name)\n",
        "    num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "    print(f\"{class_name}: {num_images} images\")"
      ],
      "metadata": {
        "id": "maqz36LzFvtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**not required** Augmenting the missing testing image"
      ],
      "metadata": {
        "id": "vx0m1VN7s-G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment the Test Dataset: select the a random image from training dataset to be in the test set"
      ],
      "metadata": {
        "id": "qFLBHDYfGn5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Specify the class that needs an image\n",
        "class_name = 'del'  # 'delete' class\n",
        "\n",
        "# Define the paths for the training and test class directories\n",
        "train_class_path = os.path.join(train_dir, class_name)\n",
        "test_class_path = os.path.join(test_dir, class_name)\n",
        "\n",
        "# Create the test class directory if it doesn't exist\n",
        "os.makedirs(test_class_path, exist_ok=True)\n",
        "\n",
        "# Get all images in the training class folder\n",
        "train_images = [img for img in os.listdir(train_class_path) if img.endswith('.jpg')]\n",
        "\n",
        "if train_images:\n",
        "    # Randomly select an image\n",
        "    random_image = random.choice(train_images)\n",
        "\n",
        "    # Define source and destination paths with the new name\n",
        "    src_path = os.path.join(train_class_path, random_image)\n",
        "    dest_path = os.path.join(test_class_path, f\"{class_name}_test.jpg\")\n",
        "\n",
        "    # Move the image from train to test and rename\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"Moved {random_image} from {class_name} training set to test set as {class_name}_test.jpg.\")\n",
        "else:\n",
        "    print(f\"No images found in {class_name} training set.\")\n"
      ],
      "metadata": {
        "id": "zl78E5odGf6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect hand landmarks in training images using mediapipe"
      ],
      "metadata": {
        "id": "38-XrctGH4ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the training images and convert it to RGB (MediaPipe expects RGB input) then MediaPipe process the image to find hand landmarks. Images Landmarks' extracted and saved as a flattened NumPy array. Then the output is saved in a specified directory with a unique filename.\n",
        "\n",
        "\n",
        "The summary file, output_summary.txt, will contain lines that describe whether hands were detected in each image processed from your dataset. Each line will specify the name of the image file and the corresponding detection result.\n",
        "\n",
        "Single Display image per Class to satisfy the output size limit\n",
        "\n",
        "9min 43 sec for 3 classes, 1h 33 min 42 mb\n",
        "add hand_landmarks.zip to /content/asl_alphabet/ after unzip to have /content/asl_alphabet/hand_landmarks/ folder available\n",
        "also output_summary.txt\n",
        "\n",
        "87000 images, detected 67520 , 77.6%"
      ],
      "metadata": {
        "id": "XaAm6qXR0Xf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize MediaPipe hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "output_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# To track the first image processed for each class\n",
        "saved_classes = set()\n",
        "output_summary = []\n",
        "\n",
        "# Process images in the training directory\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "\n",
        "    for image_name in os.listdir(class_dir):\n",
        "        image_path = os.path.join(class_dir, image_name)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image is read correctly\n",
        "        if image is None:\n",
        "            print(f\"Error reading image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Convert image to RGB for MediaPipe processing\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the image to find hands\n",
        "        results = hands.process(image_rgb)\n",
        "\n",
        "        # Check if hands are detected\n",
        "        if results.multi_hand_landmarks:\n",
        "            # Store the landmarks\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                # Draw hand landmarks on the image for visualization\n",
        "                mp.solutions.drawing_utils.draw_landmarks(\n",
        "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "                # Create a list of landmarks\n",
        "                landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
        "                landmarks_array = np.array(landmarks).flatten()  # Flatten the landmarks\n",
        "\n",
        "                # Save landmarks as a numpy array\n",
        "                output_file = os.path.join(output_dir, f\"{class_name}_{image_name}.npy\")\n",
        "                np.save(output_file, landmarks_array)\n",
        "\n",
        "            # Save the first image of each class with detected hands\n",
        "            if class_name not in saved_classes:\n",
        "                print(class_name)  # Add this line to print the class name\n",
        "                cv2_imshow(image)  # Display the image\n",
        "                cv2.waitKey(1)  # Show the image briefly without waiting for key press\n",
        "                saved_classes.add(class_name)  # Mark this class as processed\n",
        "\n",
        "            output_summary.append(f\"{image_name}: Hands detected\")\n",
        "\n",
        "        else:\n",
        "            output_summary.append(f\"{image_name}: No hands detected\")\n",
        "\n",
        "# Write summary to a file\n",
        "with open('output_summary.txt', 'w') as f:\n",
        "    for line in output_summary:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Release the MediaPipe hands object\n",
        "hands.close()\n"
      ],
      "metadata": {
        "id": "grFspfka0R2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to zip and download landmarks images to save time"
      ],
      "metadata": {
        "id": "kAm4AXSNVd_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract all landing marks then zip the output file to save time, download push to github and computer, then start training preferbly 2nd code"
      ],
      "metadata": {
        "id": "rKkKC7H1a37r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/hand_landmarks.zip /content/asl_alphabet/hand_landmarks/"
      ],
      "metadata": {
        "id": "bMgxvHmxSs1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7b4c7d-aef0-4c5f-b111-d16457a5034c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/asl_alphabet/hand_landmarks/\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/hand_landmarks.zip . -i /content/asl_alphabet/hand_landmarks/)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzip uploaded"
      ],
      "metadata": {
        "id": "Fp9Y0zE6XHo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create target directory if it doesn't exist\n",
        "target_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n"
      ],
      "metadata": {
        "id": "hOS6OGFvXG8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the file into the target directory\n",
        "with zipfile.ZipFile('/content/sample_data/hand_landmarks.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)\n",
        "\n",
        "print(\"Unzipping complete. Files extracted to:\", target_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoPKUtYyXGlv",
        "outputId": "99a3e4e6-ddbc-4aa6-86f4-3b67afa82d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping complete. Files extracted to: /content/asl_alphabet/hand_landmarks/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data set"
      ],
      "metadata": {
        "id": "xRz8z1l_aLsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that loads the .npy files and associates them with their corresponding labels (ASL alphabet, delete, nothing, or space). This will serve as your training data."
      ],
      "metadata": {
        "id": "fmAbLvfMatNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define directories\n",
        "landmark_dir = '/content/asl_alphabet/hand_landmarks/content/asl_alphabet/hand_landmarks/'\n",
        "\n",
        "# Define class labels\n",
        "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space', 'delete']\n",
        "\n",
        "# Prepare data and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for label in labels:\n",
        "    class_files = [f for f in os.listdir(landmark_dir) if f.startswith(label)]\n",
        "\n",
        "    for file_name in class_files:\n",
        "        file_path = os.path.join(landmark_dir, file_name)\n",
        "        landmarks = np.load(file_path)\n",
        "        X.append(landmarks)\n",
        "        y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Print shape of data\n",
        "print(f\"Shape of X (landmarks): {X.shape}\")\n",
        "print(f\"Shape of y (labels): {y.shape}\")\n",
        "\n",
        "# Print first few samples to validate\n",
        "print(\"\\nSample landmarks from X:\")\n",
        "print(X[:3])  # Print the first 3 samples of X\n",
        "\n",
        "print(\"\\nSample labels from y:\")\n",
        "print(y[:3])  # Print the first 3 labels from y\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Print shapes after splitting\n",
        "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_val: {X_val.shape}\")\n",
        "print(f\"Shape of y_val: {y_val.shape}\")\n",
        "\n",
        "# Print number of samples per class in the training set\n",
        "from collections import Counter\n",
        "print(\"\\nNumber of samples per class in training set:\")\n",
        "print(Counter(y_train))\n"
      ],
      "metadata": {
        "id": "BNuaCCVsasf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e10c24c-6f96-4750-9a85-cacd75f63265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (landmarks): (65625, 63)\n",
            "Shape of y (labels): (65625,)\n",
            "\n",
            "Sample landmarks from X:\n",
            "[[ 6.69406533e-01  6.56917572e-01 -6.15336035e-07  7.94970214e-01\n",
            "   5.87979674e-01 -3.77389677e-02  8.84766340e-01  4.58010435e-01\n",
            "  -4.79265116e-02  8.98268163e-01  3.37123632e-01 -6.10719845e-02\n",
            "   8.99838507e-01  2.36983418e-01 -6.81968629e-02  8.03550243e-01\n",
            "   3.34640622e-01 -3.08404025e-03  8.30370486e-01  2.45827794e-01\n",
            "  -7.42477551e-02  8.25626254e-01  3.43091846e-01 -1.15383416e-01\n",
            "   8.15997422e-01  4.36395943e-01 -1.26202241e-01  7.19204009e-01\n",
            "   3.45124155e-01 -8.17308947e-03  7.53873587e-01  2.59420335e-01\n",
            "  -8.36323202e-02  7.53197968e-01  3.79719913e-01 -1.03811517e-01\n",
            "   7.51136243e-01  4.79595870e-01 -9.44443643e-02  6.35212183e-01\n",
            "   3.68835628e-01 -2.48576161e-02  6.78202391e-01  3.01433563e-01\n",
            "  -1.06462248e-01  6.82986081e-01  4.28813070e-01 -9.32711810e-02\n",
            "   6.83951914e-01  5.20245671e-01 -5.68192005e-02  5.47591269e-01\n",
            "   4.07994390e-01 -4.39544395e-02  6.00199103e-01  3.49445969e-01\n",
            "  -1.00765266e-01  6.10268295e-01  4.42922413e-01 -8.41033384e-02\n",
            "   6.09463155e-01  5.15269637e-01 -5.23724258e-02]\n",
            " [ 2.95908332e-01  6.39042914e-01 -5.23727635e-07  4.42348957e-01\n",
            "   5.81727982e-01 -4.46345620e-02  5.57506084e-01  4.46687579e-01\n",
            "  -5.67751713e-02  5.85059106e-01  3.08615506e-01 -6.81454092e-02\n",
            "   5.80967367e-01  2.02032626e-01 -7.46344030e-02  4.61972475e-01\n",
            "   3.20480496e-01 -1.95796806e-02  4.86423850e-01  2.18434513e-01\n",
            "  -9.16264504e-02  4.68586206e-01  3.24259818e-01 -1.30827293e-01\n",
            "   4.55147654e-01  4.21626508e-01 -1.41163021e-01  3.63357246e-01\n",
            "   3.26945722e-01 -2.03061756e-02  3.84466350e-01  2.25255296e-01\n",
            "  -1.03918321e-01  3.80490899e-01  3.65751296e-01 -1.27798855e-01\n",
            "   3.80086303e-01  4.74229157e-01 -1.19753718e-01  2.71987855e-01\n",
            "   3.52429628e-01 -3.08247861e-02  2.83814579e-01  2.67955303e-01\n",
            "  -1.21141016e-01  2.94396996e-01  4.14316148e-01 -1.07668191e-01\n",
            "   3.01793694e-01  5.13537347e-01 -6.99755847e-02  1.79284513e-01\n",
            "   3.92449707e-01 -4.27464247e-02  1.88584045e-01  3.23672980e-01\n",
            "  -1.09287359e-01  2.09311813e-01  4.27457184e-01 -9.46175009e-02\n",
            "   2.25714996e-01  5.02013862e-01 -6.28112853e-02]\n",
            " [ 3.39234293e-01  8.52795005e-01 -5.53159509e-07  4.39717472e-01\n",
            "   8.00306439e-01 -2.88684983e-02  5.12700438e-01  7.05867410e-01\n",
            "  -3.65105718e-02  5.13600826e-01  6.13436103e-01 -4.59671132e-02\n",
            "   5.08511424e-01  5.36174893e-01 -5.24577200e-02  4.42279279e-01\n",
            "   6.11169815e-01  1.27492542e-03  4.49974686e-01  5.42874932e-01\n",
            "  -4.51635234e-02  4.52168941e-01  6.05796695e-01 -7.57687986e-02\n",
            "   4.52221155e-01  6.74386680e-01 -8.57629403e-02  3.76766741e-01\n",
            "   6.19789124e-01 -5.96290571e-04  3.92357141e-01  5.49126983e-01\n",
            "  -4.92493175e-02  3.95412803e-01  6.32256389e-01 -6.69711828e-02\n",
            "   3.96898240e-01  7.07651317e-01 -6.41765967e-02  3.12977880e-01\n",
            "   6.38779461e-01 -1.04302950e-02  3.32509816e-01  5.77297151e-01\n",
            "  -6.28959611e-02  3.40216786e-01  6.63668633e-01 -5.56256697e-02\n",
            "   3.44251096e-01  7.32500851e-01 -3.29656713e-02  2.47213736e-01\n",
            "   6.69434667e-01 -2.22236626e-02  2.74251670e-01  6.09224617e-01\n",
            "  -5.94455004e-02  2.86976993e-01  6.73026860e-01 -4.70793694e-02\n",
            "   2.90738225e-01  7.29516029e-01 -2.49575842e-02]]\n",
            "\n",
            "Sample labels from y:\n",
            "['A' 'A' 'A']\n",
            "\n",
            "Shape of X_train: (52500, 63)\n",
            "Shape of y_train: (52500,)\n",
            "Shape of X_val: (13125, 63)\n",
            "Shape of y_val: (13125,)\n",
            "\n",
            "Number of samples per class in training set:\n",
            "Counter({'F': 2334, 'K': 2210, 'J': 2144, 'Y': 2118, 'D': 2118, 'S': 2117, 'G': 2110, 'L': 2106, 'R': 2106, 'V': 2092, 'U': 2056, 'W': 2031, 'I': 2012, 'H': 1990, 'Z': 1984, 'T': 1924, 'O': 1922, 'E': 1887, 'A': 1881, 'X': 1815, 'B': 1814, 'Q': 1807, 'C': 1745, 'P': 1733, 'space': 1617, 'M': 1570, 'N': 1255, 'nothing': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding"
      ],
      "metadata": {
        "id": "683q0uHTd8xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Label Encoding or One-Hot Encoding to convert your class labels (y) from strings (like 'A', 'B', etc.) to numerical values before training your model. Neural networks require labels in numerical format\n",
        "\n",
        " 2 types:\n",
        " 1) Label Encoding: This method converts each class label into a unique integer (e.g., 'A' -> 0, 'B' -> 1, etc.). It's suitable if your model uses sparse categorical crossentropy as the loss function.\n",
        " For training the model with label encoding, use sparse_categorical_crossentropy as the loss function\n",
        "\n",
        " 2. One-Hot Encoding\n",
        "This method converts each label into a one-hot encoded vector (e.g., 'A' -> [1,0,0,...,0]). It's suitable for models using categorical crossentropy as the loss function.\n",
        "For training the model with one-hot encoding, use categorical_crossentropy as the loss function\n",
        "\n",
        "\n",
        "used\n",
        "Label Encode the class labels (y).\n",
        "Train the model using sparse_categorical_crossentropy loss.\n",
        "Add print statements to validate the encoding and the training process.\n"
      ],
      "metadata": {
        "id": "FGSee0uzKxGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the training and validation labels\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_val_enc = label_encoder.transform(y_val)\n",
        "\n",
        "# Print a few encoded labels to validate\n",
        "print(\"First 5 encoded labels (y_train):\", y_train_enc[:5])\n",
        "print(\"First 5 original labels (y_train):\", y_train[:5])"
      ],
      "metadata": {
        "id": "WNC4vsD2fZHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efe29e7-8ca8-4110-8b8e-5bd520d26196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 encoded labels (y_train): [ 8  1  7  9 19]\n",
            "First 5 original labels (y_train): ['I' 'B' 'H' 'J' 'T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for complex"
      ],
      "metadata": {
        "id": "On-Or4UZcaUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the training and validation labels\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_val_enc = label_encoder.transform(y_val)\n",
        "\n",
        "# Step 2: One-hot encode the labels\n",
        "y_train_onehot = to_categorical(y_train_enc, num_classes=len(labels))\n",
        "y_val_onehot = to_categorical(y_val_enc, num_classes=len(labels))"
      ],
      "metadata": {
        "id": "HQWBscpTcZep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Simple Neural Network Model"
      ],
      "metadata": {
        "id": "22WvuJPYgxsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Simple Neural Network Model\n",
        "\n",
        "In this step, you're defining the architecture of a neural network model using Keras. The Sequential model is a linear stack of layers, where you add layers one after the other.\n",
        "\n",
        "Input Layer (Dense(128, input_shape=(X_train.shape[1],), activation='relu')):\n",
        "\n",
        "Dense(128): This is a fully connected (dense) layer with 128 neurons (units). Each neuron is connected to every input feature.\n",
        "input_shape=(X_train.shape[1],): The input shape specifies how many features each input sample has. X_train.shape[1] corresponds to the number of landmarks you have per image (e.g., 63 if you have 21 hand landmarks with x, y, z coordinates each).\n",
        "activation='relu': The activation function used here is ReLU (Rectified Linear Unit), which introduces non-linearity. It outputs the input directly if positive; otherwise, it outputs zero. It helps the model learn complex patterns.\n",
        "Dropout Layer (Dropout(0.3)):\n",
        "\n",
        "This layer randomly sets 30% (0.3) of the input units to 0 during training to prevent overfitting. This encourages the model to learn more robust features.\n",
        "Second Hidden Layer (Dense(64, activation='relu')):\n",
        "\n",
        "A second fully connected layer with 64 neurons and a ReLU activation function. This allows the model to further learn and combine features.\n",
        "Another Dropout Layer:\n",
        "\n",
        "Another dropout layer is added with a 30% dropout rate to prevent overfitting in this hidden layer.\n",
        "Output Layer (Dense(len(labels), activation='softmax')):\n",
        "\n",
        "Dense(len(labels)): The output layer has as many neurons as there are classes (letters A-Z, 'nothing', 'space', 'delete'), which equals len(labels) = 29.\n",
        "activation='softmax': The softmax function is used to ensure that the output is a probability distribution over the 29 possible classes. Each neuron will output a value between 0 and 1, and the sum of the outputs will be 1 (indicating the model’s confidence for each class)."
      ],
      "metadata": {
        "id": "ItHpq5u6gjM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(labels), activation='softmax')  # Output layer with the number of classes\n",
        "])\n"
      ],
      "metadata": {
        "id": "1LATtHb_gLTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the Complexity of the Model\n",
        "\n",
        "Add more layers and neurons to allow the model to learn more complex patterns.\n",
        "For example, try adding another dense layer or increasing the number of neurons in the existing layers:\n",
        "\n",
        "Add Batch Normalization"
      ],
      "metadata": {
        "id": "y80OU39pY5m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Step 4: Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "o_AkfIH4Y5Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "7u-1nH0Lg4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, you need to compile the model by specifying:\n",
        "\n",
        "Optimizer ('adam'): The Adam optimizer is an adaptive learning rate optimization algorithm. It adjusts the learning rate based on the gradients and is one of the most commonly used optimizers for neural networks.\n",
        "\n",
        "Loss Function ('sparse_categorical_crossentropy'): This is the loss function for classification problems with multiple classes. It compares the predicted probability distribution with the true class label and calculates the error.\n",
        "\n",
        "Sparse categorical crossentropy is used when your labels are integers (i.e., label encoding), rather than one-hot encoded vectors.\n",
        "Metrics (['accuracy']): The model will track accuracy as a performance metric during training, which is the percentage of correctly classified samples."
      ],
      "metadata": {
        "id": "tWTKtFighDWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Ff0GVmrChDvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a Different Optimizer\n",
        "\n",
        "The default optimizer (adam) works well in many cases, but you can experiment with others like rmsprop or sgd."
      ],
      "metadata": {
        "id": "hD3pvKTEZDp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "TS-MbZ5wZEGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "MLTs0tUBfxYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Process: The model is trained on the dataset by iteratively updating the weights to minimize the loss.\n",
        "X_train and y_train_enc: These are your training data (landmarks and corresponding labels).\n",
        "epochs=10: The model will go through the entire training set 10 times. Each full pass over the dataset is called an epoch.\n",
        "batch_size=32: The training data is split into mini-batches of 32 samples, and the model updates its weights after each batch.\n",
        "validation_data=(X_val, y_val_enc): After each epoch, the model evaluates its performance on the validation set to track progress and ensure it's not overfitting."
      ],
      "metadata": {
        "id": "w4ifIsI-hN5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train_enc, epochs=10, batch_size=32, validation_data=(X_val, y_val_enc))\n"
      ],
      "metadata": {
        "id": "gwEa1vgHhNO0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "82d058e1-6398-47ab-8d5d-0d5ff2373f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 1.4966 - accuracy: 0.5490 - val_loss: 0.4659 - val_accuracy: 0.8834\n",
            "Epoch 2/10\n",
            "1549/1641 [===========================>..] - ETA: 0s - loss: 0.5377 - accuracy: 0.8332"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-12eec55e5610>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m--> 133\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# A new cache key will be built later when saving ConcreteFunction because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# only active captures should be saved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     lookup_func_key, _ = function_context.make_cache_key((args, kwargs),\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                          captures)\n\u001b[1;32m    338\u001b[0m     \u001b[0mconcrete_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_func_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_context.py\u001b[0m in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    146\u001b[0m   return function_cache.FunctionCacheKey(\n\u001b[1;32m    147\u001b[0m       \u001b[0mdummy_function_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       make_function_context()), signature_context.deletion_observer\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_context.py\u001b[0m in \u001b[0;36mmake_function_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0mparent_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mxla_context_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# We want to force function retracing for each different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# XLAControlFlowContext, so add `xla_context_id` to the context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase Training Epochs and Use Early Stopping\n",
        "\n",
        "Increase the number of training epochs to allow the model to learn better.\n",
        "Use early stopping to stop training when validation accuracy stops improving"
      ],
      "metadata": {
        "id": "-0Jum2gzZQbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the model\n",
        "history = model.fit(X_train, y_train_onehot,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_val, y_val_onehot),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Print summary of the model training\n",
        "print(\"Model training complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWND4UZNZRre",
        "outputId": "e36dbaa4-a714-4e3d-e16e-2fdae5d833e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1641/1641 [==============================] - 11s 6ms/step - loss: 2.5694 - accuracy: 0.2961 - val_loss: 0.9510 - val_accuracy: 0.8403\n",
            "Epoch 2/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 1.1196 - accuracy: 0.6803 - val_loss: 0.3782 - val_accuracy: 0.9345\n",
            "Epoch 3/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.6621 - accuracy: 0.8193 - val_loss: 0.2344 - val_accuracy: 0.9491\n",
            "Epoch 4/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.4700 - accuracy: 0.8726 - val_loss: 0.1771 - val_accuracy: 0.9587\n",
            "Epoch 5/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.3793 - accuracy: 0.8974 - val_loss: 0.1518 - val_accuracy: 0.9608\n",
            "Epoch 6/50\n",
            "1641/1641 [==============================] - 11s 7ms/step - loss: 0.3271 - accuracy: 0.9126 - val_loss: 0.1407 - val_accuracy: 0.9598\n",
            "Epoch 7/50\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 0.2931 - accuracy: 0.9214 - val_loss: 0.1214 - val_accuracy: 0.9690\n",
            "Epoch 8/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.2693 - accuracy: 0.9275 - val_loss: 0.1148 - val_accuracy: 0.9713\n",
            "Epoch 9/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.2458 - accuracy: 0.9336 - val_loss: 0.1043 - val_accuracy: 0.9730\n",
            "Epoch 10/50\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 0.2382 - accuracy: 0.9357 - val_loss: 0.1052 - val_accuracy: 0.9698\n",
            "Epoch 11/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.2235 - accuracy: 0.9394 - val_loss: 0.0982 - val_accuracy: 0.9737\n",
            "Epoch 12/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.2189 - accuracy: 0.9411 - val_loss: 0.0980 - val_accuracy: 0.9720\n",
            "Epoch 13/50\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 0.2120 - accuracy: 0.9420 - val_loss: 0.0918 - val_accuracy: 0.9756\n",
            "Epoch 14/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.2033 - accuracy: 0.9448 - val_loss: 0.0879 - val_accuracy: 0.9769\n",
            "Epoch 15/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1998 - accuracy: 0.9446 - val_loss: 0.1009 - val_accuracy: 0.9704\n",
            "Epoch 16/50\n",
            "1641/1641 [==============================] - 8s 5ms/step - loss: 0.1926 - accuracy: 0.9458 - val_loss: 0.0915 - val_accuracy: 0.9750\n",
            "Epoch 17/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1856 - accuracy: 0.9484 - val_loss: 0.0934 - val_accuracy: 0.9720\n",
            "Epoch 18/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1867 - accuracy: 0.9488 - val_loss: 0.0801 - val_accuracy: 0.9795\n",
            "Epoch 19/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1819 - accuracy: 0.9495 - val_loss: 0.0822 - val_accuracy: 0.9778\n",
            "Epoch 20/50\n",
            "1641/1641 [==============================] - 13s 8ms/step - loss: 0.1791 - accuracy: 0.9509 - val_loss: 0.0804 - val_accuracy: 0.9785\n",
            "Epoch 21/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1753 - accuracy: 0.9509 - val_loss: 0.0761 - val_accuracy: 0.9798\n",
            "Epoch 22/50\n",
            "1641/1641 [==============================] - 8s 5ms/step - loss: 0.1734 - accuracy: 0.9519 - val_loss: 0.0760 - val_accuracy: 0.9797\n",
            "Epoch 23/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1667 - accuracy: 0.9533 - val_loss: 0.0768 - val_accuracy: 0.9798\n",
            "Epoch 24/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1666 - accuracy: 0.9545 - val_loss: 0.0710 - val_accuracy: 0.9806\n",
            "Epoch 25/50\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 0.1653 - accuracy: 0.9551 - val_loss: 0.0742 - val_accuracy: 0.9781\n",
            "Epoch 26/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1622 - accuracy: 0.9539 - val_loss: 0.0756 - val_accuracy: 0.9779\n",
            "Epoch 27/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1578 - accuracy: 0.9557 - val_loss: 0.0690 - val_accuracy: 0.9816\n",
            "Epoch 28/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1586 - accuracy: 0.9564 - val_loss: 0.0734 - val_accuracy: 0.9803\n",
            "Epoch 29/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1532 - accuracy: 0.9576 - val_loss: 0.0708 - val_accuracy: 0.9798\n",
            "Epoch 30/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1534 - accuracy: 0.9564 - val_loss: 0.0802 - val_accuracy: 0.9736\n",
            "Epoch 31/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1504 - accuracy: 0.9589 - val_loss: 0.0629 - val_accuracy: 0.9835\n",
            "Epoch 32/50\n",
            "1641/1641 [==============================] - 11s 6ms/step - loss: 0.1528 - accuracy: 0.9568 - val_loss: 0.0654 - val_accuracy: 0.9819\n",
            "Epoch 33/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1479 - accuracy: 0.9589 - val_loss: 0.0670 - val_accuracy: 0.9815\n",
            "Epoch 34/50\n",
            "1641/1641 [==============================] - 11s 7ms/step - loss: 0.1522 - accuracy: 0.9573 - val_loss: 0.0687 - val_accuracy: 0.9810\n",
            "Epoch 35/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1473 - accuracy: 0.9582 - val_loss: 0.0660 - val_accuracy: 0.9818\n",
            "Epoch 36/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1489 - accuracy: 0.9586 - val_loss: 0.0618 - val_accuracy: 0.9822\n",
            "Epoch 37/50\n",
            "1641/1641 [==============================] - 9s 5ms/step - loss: 0.1473 - accuracy: 0.9588 - val_loss: 0.0813 - val_accuracy: 0.9744\n",
            "Epoch 38/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1417 - accuracy: 0.9601 - val_loss: 0.0641 - val_accuracy: 0.9823\n",
            "Epoch 39/50\n",
            "1641/1641 [==============================] - 9s 6ms/step - loss: 0.1414 - accuracy: 0.9602 - val_loss: 0.0626 - val_accuracy: 0.9829\n",
            "Epoch 40/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1429 - accuracy: 0.9607 - val_loss: 0.0649 - val_accuracy: 0.9825\n",
            "Epoch 41/50\n",
            "1641/1641 [==============================] - 10s 6ms/step - loss: 0.1404 - accuracy: 0.9600 - val_loss: 0.0667 - val_accuracy: 0.9812\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model"
      ],
      "metadata": {
        "id": "5AOQUSaZhaf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, the model is evaluated on the validation set to see how well it performs on unseen data.\n",
        "\n",
        "val_loss: This is the loss on the validation data after training.\n",
        "val_acc: This is the accuracy on the validation data after training.\n",
        "The model.evaluate method returns both loss and accuracy, which are printed to check how well the model is performing on validation data.\n",
        "\n",
        "Loss is not directly related to accuracy. While a lower loss typically means higher accuracy, they operate on different scales and measure different things. The loss function measures how confident the model is in its predictions, while accuracy measures the proportion of correct predictions.\n",
        "For instance, a loss of 0.1393 doesn't mean the model is 13.93% wrong. It’s just the numerical value of the error calculated using cross-entropy.\n",
        "Accuracy, on the other hand, is a percentage metric, where 0.9667 (96.67%) of the predictions were correct."
      ],
      "metadata": {
        "id": "TYx6PMDdhjBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(X_val, y_val_enc)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "J64-_Zzshjqp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "62d9b198-2f22-410c-bc03-38f21b26af1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1758, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-61e85037b459>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nValidation Loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1758, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 29) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not required:** Print Number of Samples per Class"
      ],
      "metadata": {
        "id": "mDrBHzlKhayH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step prints out the number of samples for each class in the training set. It uses the Counter class from Python’s collections module to count occurrences of each label in y_train. This is useful to check if your training set is balanced or if some classes have more samples than others."
      ],
      "metadata": {
        "id": "vDv1HBYWhoyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"\\nNumber of samples per class in training set:\")\n",
        "print(Counter(y_train))\n"
      ],
      "metadata": {
        "id": "9CF31blLhlJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model:"
      ],
      "metadata": {
        "id": "zHXxPBAMh-iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model_save_path = '/content/asl_hand_landmarks_model.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "STCMFX7siLBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514b2749-80f5-4300-95a3-8c2f7421df4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/asl_hand_landmarks_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model Later:"
      ],
      "metadata": {
        "id": "okggE-dTh-F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(model_save_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "F-9_dfr5iL8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}