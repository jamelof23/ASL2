{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install compatible versions of dependent libraries"
      ],
      "metadata": {
        "id": "UCPlIsNhCmnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages that may cause conflicts\n",
        "!pip uninstall -y tensorflow tensorflow-metadata\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install mediapipe==0.10.15\n",
        "!pip install protobuf==3.20.3\n",
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "HtUYhaNRAY8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset"
      ],
      "metadata": {
        "id": "pUOBb1nVwTyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload archive.zip to /content/sample_data/"
      ],
      "metadata": {
        "id": "5eXt1gToxNf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the Dataset"
      ],
      "metadata": {
        "id": "qFMLQcgOwgQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLe8-fFDehIb"
      },
      "outputs": [],
      "source": [
        "!unzip /content/sample_data/archive.zip -d /content/asl_alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect hand landmarks in training images using mediapipe"
      ],
      "metadata": {
        "id": "38-XrctGH4ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the training images and convert it to RGB (MediaPipe expects RGB input) then MediaPipe process the image to find hand landmarks. Images Landmarks' extracted and saved as a flattened NumPy array. Then the output is saved in a specified directory with a unique filename.\n",
        "\n",
        "\n",
        "The summary file, output_summary.txt, will contain lines that describe whether hands were detected in each image processed from your dataset. Each line will specify the name of the image file and the corresponding detection result.\n",
        "\n",
        "Single Display image per Class to satisfy the output size limit\n",
        "\n",
        "87000 images, detected 67520 , 77.6%"
      ],
      "metadata": {
        "id": "XaAm6qXR0Xf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize MediaPipe hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "output_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# To track the first image processed for each class\n",
        "saved_classes = set()\n",
        "output_summary = []\n",
        "\n",
        "# Process images in the training directory\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "\n",
        "    for image_name in os.listdir(class_dir):\n",
        "        image_path = os.path.join(class_dir, image_name)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image is read correctly\n",
        "        if image is None:\n",
        "            print(f\"Error reading image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Convert image to RGB for MediaPipe processing\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the image to find hands\n",
        "        results = hands.process(image_rgb)\n",
        "\n",
        "        # Check if hands are detected\n",
        "        if results.multi_hand_landmarks:\n",
        "            # Store the landmarks\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                # Draw hand landmarks on the image for visualization\n",
        "                mp.solutions.drawing_utils.draw_landmarks(\n",
        "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "                # Create a list of landmarks\n",
        "                landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
        "                landmarks_array = np.array(landmarks).flatten()  # Flatten the landmarks\n",
        "\n",
        "                # Save landmarks as a numpy array\n",
        "                output_file = os.path.join(output_dir, f\"{class_name}_{image_name}.npy\")\n",
        "                np.save(output_file, landmarks_array)\n",
        "\n",
        "            # Save the first image of each class with detected hands\n",
        "            if class_name not in saved_classes:\n",
        "                print(class_name)  # Add this line to print the class name\n",
        "                cv2_imshow(image)  # Display the image\n",
        "                cv2.waitKey(1)  # Show the image briefly without waiting for key press\n",
        "                saved_classes.add(class_name)  # Mark this class as processed\n",
        "\n",
        "            output_summary.append(f\"{image_name}: Hands detected\")\n",
        "\n",
        "        else:\n",
        "            output_summary.append(f\"{image_name}: No hands detected\")\n",
        "\n",
        "# Write summary to a file\n",
        "with open('output_summary.txt', 'w') as f:\n",
        "    for line in output_summary:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Release the MediaPipe hands object\n",
        "hands.close()\n"
      ],
      "metadata": {
        "id": "grFspfka0R2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save The Model**"
      ],
      "metadata": {
        "id": "R49gEepzTKKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the trained model\n",
        "model_save_path = '/content/asl_hand_landmarks_model.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "fdxaj_lATJiC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}