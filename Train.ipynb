{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf58bKpVPhsTZbTbPgs2bJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/ASL2/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install compatible versions of dependent libraries"
      ],
      "metadata": {
        "id": "UCPlIsNhCmnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages that may cause conflicts\n",
        "!pip uninstall -y tensorflow tensorflow-metadata\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install mediapipe==0.10.15\n",
        "!pip install protobuf==3.20.3\n",
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "HtUYhaNRAY8V",
        "outputId": "f9aa4b24-4829-4370-bdbb-2c1a877dec61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Found existing installation: tensorflow-metadata 1.16.0\n",
            "Uninstalling tensorflow-metadata-1.16.0:\n",
            "  Successfully uninstalled tensorflow-metadata-1.16.0\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m535.9/588.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "uIThM0T7CHI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset"
      ],
      "metadata": {
        "id": "pUOBb1nVwTyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload archive.zip to /content/sample_data/"
      ],
      "metadata": {
        "id": "5eXt1gToxNf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the Dataset"
      ],
      "metadata": {
        "id": "qFMLQcgOwgQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLe8-fFDehIb"
      },
      "outputs": [],
      "source": [
        "!unzip /content/sample_data/archive.zip -d /content/asl_alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organizing and Sorting the data"
      ],
      "metadata": {
        "id": "JIZYrVAK_Ol-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders for each class in the test directory and move the images into their respective folders. For training data its already sorted."
      ],
      "metadata": {
        "id": "GSL886HUE0xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Define the classes\n",
        "class_names = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I',\n",
        "               'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
        "               'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n",
        "\n",
        "# Create directories for each class in the test directory\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Move each test image into the appropriate class folder\n",
        "for image_name in os.listdir(test_dir):\n",
        "    if '_test.jpg' in image_name:\n",
        "        class_name = image_name.split('_')[0]  # Get the class from the image name\n",
        "        shutil.move(os.path.join(test_dir, image_name),\n",
        "                    os.path.join(test_dir, class_name, image_name))"
      ],
      "metadata": {
        "id": "THuxXIF__lSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not Required** Load the training and testing datasets"
      ],
      "metadata": {
        "id": "G9wVXmEywvdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Size\n",
        "Definition: The batch size refers to the number of training examples used in one iteration of training.\n",
        "Purpose: Instead of updating the model weights after each individual sample, which can be computationally expensive and lead to noisy updates, the model processes a batch of samples and computes the gradient based on that batch. This approach helps to stabilize the training process and can lead to faster convergence.\n",
        "Common Values: Common batch sizes are powers of 2 (like 32, 64, 128) since they can be more efficient for hardware acceleration (e.g., GPUs).\n",
        "2. Reproducibility\n",
        "Definition: Reproducibility refers to the ability to achieve the same results when you run the experiment multiple times under the same conditions.\n",
        "Purpose: In machine learning, due to the stochastic nature of algorithms (like random initialization of weights or shuffling of data), you might get different results on different runs. Setting a random seed (like seed=123 in the code) ensures that the random processes in your code (e.g., shuffling data or initializing weights) are the same every time you run the model. This way, you can replicate results and debug issues more easily.\n",
        "3. Shuffle the Dataset\n",
        "Definition: Shuffling the dataset means randomizing the order of the data samples before they are fed into the model for training.\n",
        "Purpose: Shuffling is important to ensure that the model does not learn any unintended patterns based on the order of the data (e.g., if all images of one class appear consecutively). Randomizing the input helps the model generalize better and improves performance."
      ],
      "metadata": {
        "id": "MHsYc1QK8n53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "\n",
        "# Set image parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 200, 200\n",
        "BATCH_SIZE = 32  # Adjust as needed\n",
        "SEED = 123  # Set the seed for reproducibility\n",
        "\n",
        "# Load training data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Load testing data\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Check the class names\n",
        "class_names = train_ds.class_names\n",
        "print(\"Class names:\", class_names)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KLiOxsKZyF9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding missing testing image"
      ],
      "metadata": {
        "id": "is0RhxiasJZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Count test images per class\n",
        "for class_name in class_names:\n",
        "    class_path = os.path.join(test_dir, class_name)\n",
        "    num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "    print(f\"{class_name}: {num_images} images\")"
      ],
      "metadata": {
        "id": "maqz36LzFvtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmenting the missing testing image"
      ],
      "metadata": {
        "id": "vx0m1VN7s-G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment the Test Dataset: select the a random image from training dataset to be in the test set"
      ],
      "metadata": {
        "id": "qFLBHDYfGn5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Specify the class that needs an image\n",
        "class_name = 'del'  # 'delete' class\n",
        "\n",
        "# Define the paths for the training and test class directories\n",
        "train_class_path = os.path.join(train_dir, class_name)\n",
        "test_class_path = os.path.join(test_dir, class_name)\n",
        "\n",
        "# Create the test class directory if it doesn't exist\n",
        "os.makedirs(test_class_path, exist_ok=True)\n",
        "\n",
        "# Get all images in the training class folder\n",
        "train_images = [img for img in os.listdir(train_class_path) if img.endswith('.jpg')]\n",
        "\n",
        "if train_images:\n",
        "    # Randomly select an image\n",
        "    random_image = random.choice(train_images)\n",
        "\n",
        "    # Define source and destination paths with the new name\n",
        "    src_path = os.path.join(train_class_path, random_image)\n",
        "    dest_path = os.path.join(test_class_path, f\"{class_name}_test.jpg\")\n",
        "\n",
        "    # Move the image from train to test and rename\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"Moved {random_image} from {class_name} training set to test set as {class_name}_test.jpg.\")\n",
        "else:\n",
        "    print(f\"No images found in {class_name} training set.\")\n"
      ],
      "metadata": {
        "id": "zl78E5odGf6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect hand landmarks in training images using mediapipe"
      ],
      "metadata": {
        "id": "38-XrctGH4ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the training images and convert it to RGB (MediaPipe expects RGB input) then MediaPipe process the image to find hand landmarks. Images Landmarks' extracted and saved as a flattened NumPy array. Then the output is saved in a specified directory with a unique filename.\n",
        "\n",
        "\n",
        "The summary file, output_summary.txt, will contain lines that describe whether hands were detected in each image processed from your dataset. Each line will specify the name of the image file and the corresponding detection result.\n",
        "\n",
        "Single Display image per Class to satisfy the output size limit\n",
        "\n",
        "9min 43 sec for 3 classes, 1h 33 min 42 mb\n",
        "add hand_landmarks.zip to /content/asl_alphabet/ after unzip to have /content/asl_alphabet/hand_landmarks/ folder available\n",
        "also output_summary.txt\n",
        "\n",
        "87000 images, detected 67520 , 77.6%"
      ],
      "metadata": {
        "id": "XaAm6qXR0Xf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize MediaPipe hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "output_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# To track the first image processed for each class\n",
        "saved_classes = set()\n",
        "output_summary = []\n",
        "\n",
        "# Process images in the training directory\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "\n",
        "    for image_name in os.listdir(class_dir):\n",
        "        image_path = os.path.join(class_dir, image_name)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image is read correctly\n",
        "        if image is None:\n",
        "            print(f\"Error reading image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Convert image to RGB for MediaPipe processing\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the image to find hands\n",
        "        results = hands.process(image_rgb)\n",
        "\n",
        "        # Check if hands are detected\n",
        "        if results.multi_hand_landmarks:\n",
        "            # Store the landmarks\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                # Draw hand landmarks on the image for visualization\n",
        "                mp.solutions.drawing_utils.draw_landmarks(\n",
        "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "                # Create a list of landmarks\n",
        "                landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
        "                landmarks_array = np.array(landmarks).flatten()  # Flatten the landmarks\n",
        "\n",
        "                # Save landmarks as a numpy array\n",
        "                output_file = os.path.join(output_dir, f\"{class_name}_{image_name}.npy\")\n",
        "                np.save(output_file, landmarks_array)\n",
        "\n",
        "            # Save the first image of each class with detected hands\n",
        "            if class_name not in saved_classes:\n",
        "                print(class_name)  # Add this line to print the class name\n",
        "                cv2_imshow(image)  # Display the image\n",
        "                cv2.waitKey(1)  # Show the image briefly without waiting for key press\n",
        "                saved_classes.add(class_name)  # Mark this class as processed\n",
        "\n",
        "            output_summary.append(f\"{image_name}: Hands detected\")\n",
        "\n",
        "        else:\n",
        "            output_summary.append(f\"{image_name}: No hands detected\")\n",
        "\n",
        "# Write summary to a file\n",
        "with open('output_summary.txt', 'w') as f:\n",
        "    for line in output_summary:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Release the MediaPipe hands object\n",
        "hands.close()\n"
      ],
      "metadata": {
        "id": "grFspfka0R2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to zip and download landmarks images to save time"
      ],
      "metadata": {
        "id": "kAm4AXSNVd_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract all landing marks then zip the output file to save time, download push to github and computer, then start training preferbly 2nd code"
      ],
      "metadata": {
        "id": "rKkKC7H1a37r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/hand_landmarks.zip /content/asl_alphabet/hand_landmarks/"
      ],
      "metadata": {
        "id": "bMgxvHmxSs1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data set"
      ],
      "metadata": {
        "id": "xRz8z1l_aLsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that loads the .npy files and associates them with their corresponding labels (ASL alphabet, delete, nothing, or space). This will serve as your training data."
      ],
      "metadata": {
        "id": "fmAbLvfMatNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define directories\n",
        "landmark_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space', 'delete']\n",
        "\n",
        "# Prepare data and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for label in labels:\n",
        "    class_files = [f for f in os.listdir(landmark_dir) if f.startswith(label)]\n",
        "\n",
        "    for file_name in class_files:\n",
        "        file_path = os.path.join(landmark_dir, file_name)\n",
        "        landmarks = np.load(file_path)\n",
        "        X.append(landmarks)\n",
        "        y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Print shape of data\n",
        "print(f\"Shape of X (landmarks): {X.shape}\")\n",
        "print(f\"Shape of y (labels): {y.shape}\")\n",
        "\n",
        "# Print first few samples to validate\n",
        "print(\"\\nSample landmarks from X:\")\n",
        "print(X[:3])  # Print the first 3 samples of X\n",
        "\n",
        "print(\"\\nSample labels from y:\")\n",
        "print(y[:3])  # Print the first 3 labels from y\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Print shapes after splitting\n",
        "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_val: {X_val.shape}\")\n",
        "print(f\"Shape of y_val: {y_val.shape}\")\n",
        "\n",
        "# Print number of samples per class in the training set\n",
        "from collections import Counter\n",
        "print(\"\\nNumber of samples per class in training set:\")\n",
        "print(Counter(y_train))\n"
      ],
      "metadata": {
        "id": "BNuaCCVsasf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding"
      ],
      "metadata": {
        "id": "683q0uHTd8xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Label Encoding or One-Hot Encoding to convert your class labels (y) from strings (like 'A', 'B', etc.) to numerical values before training your model. Neural networks require labels in numerical format\n",
        "\n",
        " 2 types:\n",
        " 1) Label Encoding: This method converts each class label into a unique integer (e.g., 'A' -> 0, 'B' -> 1, etc.). It's suitable if your model uses sparse categorical crossentropy as the loss function.\n",
        " For training the model with label encoding, use sparse_categorical_crossentropy as the loss function\n",
        "\n",
        " 2. One-Hot Encoding\n",
        "This method converts each label into a one-hot encoded vector (e.g., 'A' -> [1,0,0,...,0]). It's suitable for models using categorical crossentropy as the loss function.\n",
        "For training the model with one-hot encoding, use categorical_crossentropy as the loss function\n",
        "\n",
        "\n",
        "used\n",
        "Label Encode the class labels (y).\n",
        "Train the model using sparse_categorical_crossentropy loss.\n",
        "Add print statements to validate the encoding and the training process.\n"
      ],
      "metadata": {
        "id": "FGSee0uzKxGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the training and validation labels\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_val_enc = label_encoder.transform(y_val)\n",
        "\n",
        "# Print a few encoded labels to validate\n",
        "print(\"First 5 encoded labels (y_train):\", y_train_enc[:5])\n",
        "print(\"First 5 original labels (y_train):\", y_train[:5])"
      ],
      "metadata": {
        "id": "WNC4vsD2fZHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Simple Neural Network Model"
      ],
      "metadata": {
        "id": "22WvuJPYgxsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Simple Neural Network Model\n",
        "\n",
        "In this step, you're defining the architecture of a neural network model using Keras. The Sequential model is a linear stack of layers, where you add layers one after the other.\n",
        "\n",
        "Input Layer (Dense(128, input_shape=(X_train.shape[1],), activation='relu')):\n",
        "\n",
        "Dense(128): This is a fully connected (dense) layer with 128 neurons (units). Each neuron is connected to every input feature.\n",
        "input_shape=(X_train.shape[1],): The input shape specifies how many features each input sample has. X_train.shape[1] corresponds to the number of landmarks you have per image (e.g., 63 if you have 21 hand landmarks with x, y, z coordinates each).\n",
        "activation='relu': The activation function used here is ReLU (Rectified Linear Unit), which introduces non-linearity. It outputs the input directly if positive; otherwise, it outputs zero. It helps the model learn complex patterns.\n",
        "Dropout Layer (Dropout(0.3)):\n",
        "\n",
        "This layer randomly sets 30% (0.3) of the input units to 0 during training to prevent overfitting. This encourages the model to learn more robust features.\n",
        "Second Hidden Layer (Dense(64, activation='relu')):\n",
        "\n",
        "A second fully connected layer with 64 neurons and a ReLU activation function. This allows the model to further learn and combine features.\n",
        "Another Dropout Layer:\n",
        "\n",
        "Another dropout layer is added with a 30% dropout rate to prevent overfitting in this hidden layer.\n",
        "Output Layer (Dense(len(labels), activation='softmax')):\n",
        "\n",
        "Dense(len(labels)): The output layer has as many neurons as there are classes (letters A-Z, 'nothing', 'space', 'delete'), which equals len(labels) = 29.\n",
        "activation='softmax': The softmax function is used to ensure that the output is a probability distribution over the 29 possible classes. Each neuron will output a value between 0 and 1, and the sum of the outputs will be 1 (indicating the model’s confidence for each class)."
      ],
      "metadata": {
        "id": "ItHpq5u6gjM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(labels), activation='softmax')  # Output layer with the number of classes\n",
        "])\n"
      ],
      "metadata": {
        "id": "1LATtHb_gLTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "7u-1nH0Lg4Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, you need to compile the model by specifying:\n",
        "\n",
        "Optimizer ('adam'): The Adam optimizer is an adaptive learning rate optimization algorithm. It adjusts the learning rate based on the gradients and is one of the most commonly used optimizers for neural networks.\n",
        "\n",
        "Loss Function ('sparse_categorical_crossentropy'): This is the loss function for classification problems with multiple classes. It compares the predicted probability distribution with the true class label and calculates the error.\n",
        "\n",
        "Sparse categorical crossentropy is used when your labels are integers (i.e., label encoding), rather than one-hot encoded vectors.\n",
        "Metrics (['accuracy']): The model will track accuracy as a performance metric during training, which is the percentage of correctly classified samples."
      ],
      "metadata": {
        "id": "tWTKtFighDWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Ff0GVmrChDvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "MLTs0tUBfxYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Process: The model is trained on the dataset by iteratively updating the weights to minimize the loss.\n",
        "X_train and y_train_enc: These are your training data (landmarks and corresponding labels).\n",
        "epochs=10: The model will go through the entire training set 10 times. Each full pass over the dataset is called an epoch.\n",
        "batch_size=32: The training data is split into mini-batches of 32 samples, and the model updates its weights after each batch.\n",
        "validation_data=(X_val, y_val_enc): After each epoch, the model evaluates its performance on the validation set to track progress and ensure it's not overfitting."
      ],
      "metadata": {
        "id": "w4ifIsI-hN5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train_enc, epochs=10, batch_size=32, validation_data=(X_val, y_val_enc))\n"
      ],
      "metadata": {
        "id": "gwEa1vgHhNO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model"
      ],
      "metadata": {
        "id": "5AOQUSaZhaf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, the model is evaluated on the validation set to see how well it performs on unseen data.\n",
        "\n",
        "val_loss: This is the loss on the validation data after training.\n",
        "val_acc: This is the accuracy on the validation data after training.\n",
        "The model.evaluate method returns both loss and accuracy, which are printed to check how well the model is performing on validation data.\n",
        "\n",
        "Loss is not directly related to accuracy. While a lower loss typically means higher accuracy, they operate on different scales and measure different things. The loss function measures how confident the model is in its predictions, while accuracy measures the proportion of correct predictions.\n",
        "For instance, a loss of 0.1393 doesn't mean the model is 13.93% wrong. It’s just the numerical value of the error calculated using cross-entropy.\n",
        "Accuracy, on the other hand, is a percentage metric, where 0.9667 (96.67%) of the predictions were correct."
      ],
      "metadata": {
        "id": "TYx6PMDdhjBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(X_val, y_val_enc)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "J64-_Zzshjqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not required:** Print Number of Samples per Class"
      ],
      "metadata": {
        "id": "mDrBHzlKhayH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step prints out the number of samples for each class in the training set. It uses the Counter class from Python’s collections module to count occurrences of each label in y_train. This is useful to check if your training set is balanced or if some classes have more samples than others."
      ],
      "metadata": {
        "id": "vDv1HBYWhoyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"\\nNumber of samples per class in training set:\")\n",
        "print(Counter(y_train))\n"
      ],
      "metadata": {
        "id": "9CF31blLhlJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model:"
      ],
      "metadata": {
        "id": "zHXxPBAMh-iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model_save_path = '/content/asl_hand_landmarks_model.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "STCMFX7siLBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model Later:"
      ],
      "metadata": {
        "id": "okggE-dTh-F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(model_save_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "F-9_dfr5iL8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}