{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMosYJWT7ZpM8PQJjHIiu4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/ASL2/blob/main/ASL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install compatible versions of dependent libraries"
      ],
      "metadata": {
        "id": "UCPlIsNhCmnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages that may cause conflicts\n",
        "!pip uninstall -y tensorflow tensorflow-metadata\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install mediapipe==0.10.15\n",
        "!pip install protobuf==3.20.3\n",
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "HtUYhaNRAY8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"MediaPipe version:\", mp.__version__)\n"
      ],
      "metadata": {
        "id": "uIThM0T7CHI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset"
      ],
      "metadata": {
        "id": "pUOBb1nVwTyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload archive.zip to /content/sample_data/"
      ],
      "metadata": {
        "id": "5eXt1gToxNf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the Dataset"
      ],
      "metadata": {
        "id": "qFMLQcgOwgQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLe8-fFDehIb"
      },
      "outputs": [],
      "source": [
        "!unzip /content/sample_data/archive.zip -d /content/asl_alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organizing and Sorting the data"
      ],
      "metadata": {
        "id": "JIZYrVAK_Ol-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders for each class in the test directory and move the images into their respective folders. For training data its already sorted."
      ],
      "metadata": {
        "id": "GSL886HUE0xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Define the classes\n",
        "class_names = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I',\n",
        "               'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
        "               'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space']\n",
        "\n",
        "# Create directories for each class in the test directory\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Move each test image into the appropriate class folder\n",
        "for image_name in os.listdir(test_dir):\n",
        "    if '_test.jpg' in image_name:\n",
        "        class_name = image_name.split('_')[0]  # Get the class from the image name\n",
        "        shutil.move(os.path.join(test_dir, image_name),\n",
        "                    os.path.join(test_dir, class_name, image_name))"
      ],
      "metadata": {
        "id": "THuxXIF__lSR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not Required** Load the training and testing datasets"
      ],
      "metadata": {
        "id": "G9wVXmEywvdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Size\n",
        "Definition: The batch size refers to the number of training examples used in one iteration of training.\n",
        "Purpose: Instead of updating the model weights after each individual sample, which can be computationally expensive and lead to noisy updates, the model processes a batch of samples and computes the gradient based on that batch. This approach helps to stabilize the training process and can lead to faster convergence.\n",
        "Common Values: Common batch sizes are powers of 2 (like 32, 64, 128) since they can be more efficient for hardware acceleration (e.g., GPUs).\n",
        "2. Reproducibility\n",
        "Definition: Reproducibility refers to the ability to achieve the same results when you run the experiment multiple times under the same conditions.\n",
        "Purpose: In machine learning, due to the stochastic nature of algorithms (like random initialization of weights or shuffling of data), you might get different results on different runs. Setting a random seed (like seed=123 in the code) ensures that the random processes in your code (e.g., shuffling data or initializing weights) are the same every time you run the model. This way, you can replicate results and debug issues more easily.\n",
        "3. Shuffle the Dataset\n",
        "Definition: Shuffling the dataset means randomizing the order of the data samples before they are fed into the model for training.\n",
        "Purpose: Shuffling is important to ensure that the model does not learn any unintended patterns based on the order of the data (e.g., if all images of one class appear consecutively). Randomizing the input helps the model generalize better and improves performance."
      ],
      "metadata": {
        "id": "MHsYc1QK8n53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "\n",
        "# Set image parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 200, 200\n",
        "BATCH_SIZE = 32  # Adjust as needed\n",
        "SEED = 123  # Set the seed for reproducibility\n",
        "\n",
        "# Load training data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Load testing data\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    seed=SEED  # Set the seed for reproducibility\n",
        ")\n",
        "\n",
        "# Check the class names\n",
        "class_names = train_ds.class_names\n",
        "print(\"Class names:\", class_names)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KLiOxsKZyF9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding missing testing image"
      ],
      "metadata": {
        "id": "is0RhxiasJZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Count test images per class\n",
        "for class_name in class_names:\n",
        "    class_path = os.path.join(test_dir, class_name)\n",
        "    num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "    print(f\"{class_name}: {num_images} images\")"
      ],
      "metadata": {
        "id": "maqz36LzFvtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmenting the missing testing image"
      ],
      "metadata": {
        "id": "vx0m1VN7s-G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment the Test Dataset: select the a random image from training dataset to be in the test set"
      ],
      "metadata": {
        "id": "qFLBHDYfGn5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "test_dir = '/content/asl_alphabet/asl_alphabet_test/asl_alphabet_test/'\n",
        "\n",
        "# Specify the class that needs an image\n",
        "class_name = 'del'  # 'delete' class\n",
        "\n",
        "# Define the paths for the training and test class directories\n",
        "train_class_path = os.path.join(train_dir, class_name)\n",
        "test_class_path = os.path.join(test_dir, class_name)\n",
        "\n",
        "# Create the test class directory if it doesn't exist\n",
        "os.makedirs(test_class_path, exist_ok=True)\n",
        "\n",
        "# Get all images in the training class folder\n",
        "train_images = [img for img in os.listdir(train_class_path) if img.endswith('.jpg')]\n",
        "\n",
        "if train_images:\n",
        "    # Randomly select an image\n",
        "    random_image = random.choice(train_images)\n",
        "\n",
        "    # Define source and destination paths with the new name\n",
        "    src_path = os.path.join(train_class_path, random_image)\n",
        "    dest_path = os.path.join(test_class_path, f\"{class_name}_test.jpg\")\n",
        "\n",
        "    # Move the image from train to test and rename\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"Moved {random_image} from {class_name} training set to test set as {class_name}_test.jpg.\")\n",
        "else:\n",
        "    print(f\"No images found in {class_name} training set.\")\n"
      ],
      "metadata": {
        "id": "zl78E5odGf6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect hand landmarks in training images using mediapipe"
      ],
      "metadata": {
        "id": "38-XrctGH4ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the training images and convert it to RGB (MediaPipe expects RGB input) then MediaPipe process the image to find hand landmarks. Images Landmarks' extracted and saved as a flattened NumPy array. Then the output is saved in a specified directory with a unique filename.\n",
        "\n",
        "\n",
        "The summary file, output_summary.txt, will contain lines that describe whether hands were detected in each image processed from your dataset. Each line will specify the name of the image file and the corresponding detection result.\n",
        "\n",
        "Single Display image per Class to satisfy the output size limit\n",
        "\n",
        "9min 43 sec for 3 classes"
      ],
      "metadata": {
        "id": "XaAm6qXR0Xf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize MediaPipe hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
        "output_dir = '/content/asl_alphabet/hand_landmarks/'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# To track the first image processed for each class\n",
        "saved_classes = set()\n",
        "output_summary = []\n",
        "\n",
        "# Process images in the training directory\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "\n",
        "    for image_name in os.listdir(class_dir):\n",
        "        image_path = os.path.join(class_dir, image_name)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image is read correctly\n",
        "        if image is None:\n",
        "            print(f\"Error reading image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Convert image to RGB for MediaPipe processing\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the image to find hands\n",
        "        results = hands.process(image_rgb)\n",
        "\n",
        "        # Check if hands are detected\n",
        "        if results.multi_hand_landmarks:\n",
        "            # Store the landmarks\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                # Draw hand landmarks on the image for visualization\n",
        "                mp.solutions.drawing_utils.draw_landmarks(\n",
        "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "                # Create a list of landmarks\n",
        "                landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
        "                landmarks_array = np.array(landmarks).flatten()  # Flatten the landmarks\n",
        "\n",
        "                # Save landmarks as a numpy array\n",
        "                output_file = os.path.join(output_dir, f\"{class_name}_{image_name}.npy\")\n",
        "                np.save(output_file, landmarks_array)\n",
        "\n",
        "            # Save the first image of each class with detected hands\n",
        "            if class_name not in saved_classes:\n",
        "                print(class_name)  # Add this line to print the class name\n",
        "                cv2_imshow(image)  # Display the image\n",
        "                cv2.waitKey(1)  # Show the image briefly without waiting for key press\n",
        "                saved_classes.add(class_name)  # Mark this class as processed\n",
        "\n",
        "            output_summary.append(f\"{image_name}: Hands detected\")\n",
        "\n",
        "        else:\n",
        "            output_summary.append(f\"{image_name}: No hands detected\")\n",
        "\n",
        "# Write summary to a file\n",
        "with open('output_summary.txt', 'w') as f:\n",
        "    for line in output_summary:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Release the MediaPipe hands object\n",
        "hands.close()\n"
      ],
      "metadata": {
        "id": "grFspfka0R2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model After extracting landmarks from images"
      ],
      "metadata": {
        "id": "FGSee0uzKxGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Your Model\n",
        "After extracting landmarks from your images, you can use the resulting files (e.g., .npy) as input features for training your model. The training labels can still be derived from your original dataset."
      ],
      "metadata": {
        "id": "veFwKs3O0i9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Model\n",
        "You can now build and train your model using frameworks like TensorFlow or PyTorch. For example:"
      ],
      "metadata": {
        "id": "Q4JuimGg0oJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load your data (X: landmarks, y: labels)\n",
        "# Implement your data loading logic here\n",
        "\n",
        "# Build your model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(landmark_dim,)),  # Adjust landmark_dim\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')  # num_classes based on your dataset\n",
        "])\n",
        "\n",
        "# Compile and train your model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=10, batch_size=32)  # Adjust epochs and batch_size as needed\n"
      ],
      "metadata": {
        "id": "ayPiNo8c0o5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "another trsining code from landmark folder"
      ],
      "metadata": {
        "id": "wg-KAGgyZXAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Example paths\n",
        "landmarks_dir = '/content/asl_alphabet/hand_landmarks/'  # Directory with landmark .npy files\n",
        "labels = []  # List to store class labels\n",
        "landmarks = []  # List to store landmark data\n",
        "\n",
        "# Load landmarks and corresponding labels\n",
        "for class_name in os.listdir(landmarks_dir):\n",
        "    class_dir = os.path.join(landmarks_dir, class_name)\n",
        "\n",
        "    for npy_file in os.listdir(class_dir):\n",
        "        npy_path = os.path.join(class_dir, npy_file)\n",
        "\n",
        "        # Load landmark data\n",
        "        landmark_data = np.load(npy_path)\n",
        "        landmarks.append(landmark_data)\n",
        "        labels.append(class_name)  # Append the class label (e.g., letter represented)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(landmarks)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Create a TensorFlow dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "train_ds = train_ds.shuffle(len(X)).batch(32)  # Shuffle and batch the dataset\n",
        "\n",
        "# Now you can use train_ds to train your model\n"
      ],
      "metadata": {
        "id": "CaBWal0ZZT8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to zip and download landmarks images to save time"
      ],
      "metadata": {
        "id": "kAm4AXSNVd_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/hand_landmarks.zip /content/asl_alphabet/hand_landmarks/"
      ],
      "metadata": {
        "id": "bMgxvHmxSs1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract all landing marks then zip the output file to save time, download push to github and computer, then start training preferbly 2nd code"
      ],
      "metadata": {
        "id": "rKkKC7H1a37r"
      }
    }
  ]
}